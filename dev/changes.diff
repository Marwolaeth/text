diff --git a/.Rhistory b/.Rhistory
index f7da856..5eeecc1 100644
--- a/.Rhistory
+++ b/.Rhistory
@@ -1,512 +1,512 @@
-.rs.restartR()
-#
-# .rs.restartR()
-#help(textrpp_initialize)
-textrpp_initialize(
-condaenv = "berttopic2",
-refresh_settings = TRUE
-)
-# Load and prepare data
-data1 <- Language_based_assessment_data_8[c("satisfactiontexts", "swlstotal")]
-colnames(data1) <- c("text", "score")
-data2 <- Language_based_assessment_data_8[c("harmonytexts", "hilstotal")]
-colnames(data2) <- c("text", "score")
-data3 <- Language_based_assessment_data_3_100[1:2]
-colnames(data3) <- c("text", "score")
-data <- dplyr::bind_rows(data1, data2, data3)
-# Create BERTopic model trained on data["text"] help(textTopics)
-bert_model <- text::textTopics(data = data,
-variable_name = "text",
-embedding_model = "distilroberta",
-min_df = 2,
-set_seed = 42,
-save_dir="./results")
-testthat::expect_equal(bert_model$preds$t_1[2],
-.1115696,
-tolerance = 0.0001)
-#  Testing  how individual topics are associated with "score"
-test2 <- text::textTopicsTest(
-model = bert_model,
-pred_var_x = "score",
-test_method = "linear_regression"
-)
-#  Testing  how individual topics are associated with "score"
-help(textTopicsTest)
-test2 <- text::textTopicsTest(
-model = bert_model,
-pred_var_x = "score",
-test_method = "linear_regression"
-)
-testthat::expect_equal(test2[[1]]$test$x.score.estimate[1],
-.1056764,
-tolerance = 0.0001)
-test2
-testthat::expect_equal(test2$test$x.score.estimate[1],
-.1056764,
-tolerance = 0.0001)
-plots <- text::textTopicsWordcloud(
-model = bert_model,
-test = test2,
-figure_format = "png",
-save_dir = "./results"
-)
-devtools::document()
-devtools::build()
-#  Testing  how individual topics are associated with "score"
-test2 <- text::textTopicsTest(
-model = bert_model,
-pred_var_x = "score",
-test_method = "linear_regression"
-)
-#  Testing  how individual topics are associated with "score"
-test2 <- text::textTopicsTest(
-model = bert_model,
-pred_var_x = "score",
-test_method = "linear_regression"
-)
-testthat::expect_equal(test2$test$x.score.estimate[1],
-.1056764,
-tolerance = 0.0001)
-plots <- text::textTopicsWordcloud(
-model = bert_model,
-test = test2,
-figure_format = "png",
-save_dir = "./results"
-)
-#  Testing  how individual topics are associated with "score"
-test2 <- text::textTopicsTest(
-model = bert_model,
-pred_var_x = "score",
-test_method = "linear_regression"
-)
-testthat::expect_equal(test2$test$x.score.estimate[1],
-.1056764,
-tolerance = 0.0001)
-plots <- text::textTopicsWordcloud(
-model = bert_model,
-test = test2,
-figure_format = "png",
-save_dir = "./results"
-)
-plots <- text::textTopicsWordcloud(
-model = bert_model,
-test = test2,
-p_threshold = 0.05,
-figure_format = "png",
-save_dir = "./results"
-)
-plots <- text::textTopicsWordcloud(
-model = bert_model,
-test = test2,
-p_threshold = 0.05,
-figure_format = "png",
-save_dir = "./results"
-)
-plots <- text::textTopicsWordcloud(
-model = bert_model,
-save_dir = "./results",
-figure_format = "png"
-)
-plots <- text::textTopicsWordcloud(
-model = bert_model,
-test = test2,
-p_threshold = 0.05,
-figure_format = "png",
-save_dir = "./results"
-)
-plots <- text::textTopicsWordcloud(
-model = bert_model,
-test = test2,
-p_alpha = 0.05,
-figure_format = "png",
-save_dir = "./results"
-)
-devtools::install()
-devtools::document()
-.rs.restartR()
-devtools::document()
-devtools::build()
-rcmdcheck::rcmdcheck()
-devtools::document()
-rcmdcheck::rcmdcheck()
-devtools::document()
-devtools::document()
-devtools::build()
-rcmdcheck::rcmdcheck()
-devtools::document()
-pkgload::dev_help('textTopicsWordcloud')
-test2$test$x.score.estimate[1]
-devtools::build()
-devtools::document()
-text = "John, please get that article on www.linkedin.com to me by 5:00PM
-on Jan 9th 2012. 4:00 would be ideal, actually. If you have any
-questions, You can reach me at (519)-236-2723x341 or get in touch with
-my associate at harold.smith@gmail.com or on Twitter at @harold_smith__"
-text_cleaned <- textCleaning(text)
-text_cleaned
-paste0("John, please get that article on www.linkedin.com to me by 5:00PM on",
-"Jan 9th 2012. 4:00 would be ideal, actually. If you have any ",
-"questions, You can reach me at (519)-236-2723x341 or get in touch with ",
-"my associate at harold.smith@gmail.com or on Twitter at @harold_smith__")
-text = paste0("John, please get that article on www.linkedin.com to me by 5:00PM on",
-"Jan 9th 2012. 4:00 would be ideal, actually. If you have any ",
-"questions, You can reach me at (519)-236-2723x341 or get in touch with ",
-"my associate at harold.smith@gmail.com or on Twitter at @harold_smith__")
-text
-text = paste0("John, please get that article on www.linkedin.com to me by 5:00PM on",
-"Jan 9th 2012. 4:00 would be ideal, actually. If you have any ",
-"questions, You can reach me at (519)-236-2723x341 or get in touch with ",
-"my associate at harold.smith@gmail.com or on Twitter at @harold_smith__")
-text_cleaned <- textCleaning(text)
-text_cleaned
-text = paste0("John, please get that article on www.linkedin.com to me by 5:00PM on ",
-"Jan 9th 2012. 4:00 would be ideal, actually. If you have any ",
-"questions, You can reach me at (519)-236-2723x341 or get in touch with ",
-"my associate at harold.smith@gmail.com or on Twitter at @harold_smith__ !")
-text_cleaned <- textCleaning(text)
-# List of placeholders to check
-placeholders <- c("<URL>", "<TIME>", "<DATE_STRING>", "<PHONE_NUMBER>", "<EMAIL_ADDRESS>", "<@_SYMBOL>")
-# Check if all placeholders are in the text
-all_present <- all(sapply(placeholders, function(placeholder) grepl(placeholder, text)))
-all_present
-# Test that all placeholders are in the text
-for (placeholder in placeholders) {
-textthat::expect_equal(
-grepl(placeholder, text),
-TRUE,
-info = paste("Placeholder", placeholder, "is missing.")
-)
-}
-testthat::expect_equal(
-grepl(placeholder, text),
-TRUE,
-info = paste("Placeholder", placeholder, "is missing.")
-)
-testthat::expect_equal(
-grepl(placeholder, text_cleaned),
-TRUE,
-info = paste("Placeholder", placeholder, "is missing.")
-)
-text_cleaned
-# Test that all placeholders are in the text
-for (placeholder in placeholders) {
-testthat::expect_equal(
-grepl(placeholder, text_cleaned),
-TRUE,
-info = paste("Placeholder", placeholder, "is missing.")
-)
-}
-# List of placeholders to check
-placeholders <- c("<URL>", "<TIME>", "<DATE_STRING>",
-"<PHONE_NUMBER>", "<EMAIL_ADDRESS>",
-"<adfasfasf>",
-"<@_SYMBOL>")
-# Test that all placeholders are in the text
-for (placeholder in placeholders) {
-testthat::expect_equal(
-grepl(placeholder, text_cleaned),
-TRUE,
-info = paste("Placeholder", placeholder, "is missing.")
-)
-}
-# List of placeholders to check
-placeholders <- c("<URL>", "<TIME>", "<DATE_STRING>",
-"<PHONE_NUMBER>", "<EMAIL_ADDRESS>",
-#"<adfasfasf>",
-"<@_SYMBOL>")
-# Test that all placeholders are in the text
-for (placeholder in placeholders) {
-testthat::expect_equal(
-grepl(placeholder, text_cleaned),
-TRUE,
-info = paste("Placeholder", placeholder, "is missing.")
-)
-}
-devtools::document()
-devtools::document()
-rcmdcheck::rcmdcheck()
-pkgdown::build_site(new_process = FALSE)
-devtools::build()
-devtools::document()
-pkgdown::build_site(new_process = FALSE)
-install.packages("topics")
-install.packages("topics")
-# Load and prepare data
-data1 <- Language_based_assessment_data_8[c("satisfactiontexts", "swlstotal")]
-colnames(data1) <- c("text", "score")
-data2 <- Language_based_assessment_data_8[c("harmonytexts", "hilstotal")]
-colnames(data2) <- c("text", "score")
-data3 <- Language_based_assessment_data_3_100[1:2]
-colnames(data3) <- c("text", "score")
-data <- dplyr::bind_rows(data1, data2, data3)
-# Create BERTopic model trained on data["text"] help(textTopics)
-bert_model <- text::textTopics(data = data,
-variable_name = "text",
-embedding_model = "distilroberta",
-min_df = 2,
-set_seed = 42,
-save_dir="./results")
-#install.packages("topics")
-#
-# .rs.restartR()
-#help(textrpp_initialize)
-textrpp_initialize(
-condaenv = "berttopic2",
-refresh_settings = TRUE
-)
-.rs.restartR()
-#install.packages("topics")
-#
-# .rs.restartR()
-#help(textrpp_initialize)
-textrpp_initialize(
-condaenv = "berttopic2",
-refresh_settings = TRUE
-)
-library(text)
-#install.packages("topics")
-#
-# .rs.restartR()
-#help(textrpp_initialize)
-textrpp_initialize(
-condaenv = "berttopic2",
-refresh_settings = TRUE
-)
-# Create BERTopic model trained on data["text"] help(textTopics)
-bert_model <- text::textTopics(data = data,
-variable_name = "text",
-embedding_model = "distilroberta",
-min_df = 2,
-set_seed = 42,
-save_dir="./results")
-testthat::expect_equal(bert_model$preds$t_1[2],
-.1115696,
-tolerance = 0.0001)
-#  Testing  how individual topics are associated with "score"
-test2 <- text::textTopicsTest(
-model = bert_model,
-pred_var_x = "score",
-test_method = "linear_regression"
-)
-plots <- text::textTopicsWordcloud(
-model = bert_model,
-test = test2,
-# p_alpha = 0.05,
-figure_format = "png",
-save_dir = "./results"
-)
-plots <- text::textTopicsWordcloud(
-model = bert_model,
-test = test2,
-# p_alpha = 0.05,
-figure_format = "png",
-seed = 42,
-save_dir = "./results"
-)
-plots <- text::textTopicsWordcloud(
-model = bert_model,
-save_dir = "./results",
-figure_format = "png",
-seed = 42,
-)
-unlink("./results", recursive = TRUE)
-save_dir_temp <- tempdir()
-# Load and prepare data
-data1 <- Language_based_assessment_data_8[c("satisfactiontexts", "swlstotal")]
-colnames(data1) <- c("text", "score")
-data2 <- Language_based_assessment_data_8[c("harmonytexts", "hilstotal")]
-colnames(data2) <- c("text", "score")
-data3 <- Language_based_assessment_data_3_100[1:2]
-colnames(data3) <- c("text", "score")
-data <- dplyr::bind_rows(data1, data2, data3)
-# Create BERTopic model trained on data["text"] help(textTopics)
-bert_model <- text::textTopics(data = data,
-variable_name = "text",
-embedding_model = "distilroberta",
-min_df = 2,
-set_seed = 42,
-save_dir= save_dir_temp)
-testthat::expect_equal(bert_model$preds$t_1[2],
-.1115696,
-tolerance = 0.0001)
-#  Testing  how individual topics are associated with "score"
-test2 <- text::textTopicsTest(
-model = bert_model,
-pred_var_x = "score",
-test_method = "linear_regression"
-)
-plots <- text::textTopicsWordcloud(
-model = bert_model,
-test = test2,
-# p_alpha = 0.05,
-figure_format = "png",
-seed = 42,
-save_dir = save_dir_temp
-)
-plots <- text::textTopicsWordcloud(
-model = bert_model,
-save_dir = save_dir_temp,
-figure_format = "png",
-seed = 42,
-)
-predictions_powerprevsen_4 <- textPredict(
-texts = schone_training$text_english[1:50],
-model_info = "implicitpower_roberta23_previoussentence_nilsson2024",
-story_id = schone_training$story_id_num[1:50],
-participant_id = schone_training$participant_id[1:50],
-dataset_to_merge_predictions = schone_training[1:50,],
-previous_sentence = T)
-# Manually recreate the dataset for 20 participants and 80 stories
-PSE_stories <- tibble(data.frame(
-Unnamed_0 = 1:62,
-Participant_ID = rep(paste0('P', sprintf('%02d', 1:31)), each = 2),  # 31 participants, each contributing 2 stories
-Picture_ID = rep(c('IMG001', 'IMG002'), times = 31),
-Story_Text = c(
-"In the heart of the old forest, there was a place where magic still thrived. The trees whispered ancient secrets, and the ground pulsed with life.",
-"The sun was shining brightly as the family arrived at the park for a picnic. Laughter filled the air as the children ran to play.",
-"Emily loved to paint. One day, she found a magical brush that brought her paintings to life. Her world was suddenly filled with vibrant creatures.",
-"A young boy named Jake let go of his balloon, watching it soar into the sky. He smiled as it drifted away, imagining where it would go.",
-"The sea was calm, and the boat gently rocked as the fisherman cast his net. He hummed an old tune, feeling at peace with the world.",
-"The library was quiet except for the soft sound of turning pages. A girl sat in the corner, lost in a world of dragons and knights.",
-"The mountain path was steep, but they pressed on, determined to reach the summit. Clouds gathered overhead, and a storm was brewing.",
-"The train sped through the countryside, and the passengers stared out at the passing fields. In one car, a young woman scribbled furiously in a notebook.",
-"The city streets were alive with the buzz of activity. People hurried by, oblivious to the man playing a sad tune on his guitar.",
-"The forest was dark and dense, but they felt a sense of wonder as they ventured deeper. Something ancient watched them from the shadows.",
-"The campfire crackled as stories were shared. The group huddled closer, the cold night air nipping at their faces.",
-"The waterfall roared as it plunged into the river below. The mist hung in the air, creating a rainbow in the afternoon light.",
-"The old house creaked with every step. Dust hung in the air, and the furniture was covered in white sheets, as though it had been abandoned for years.",
-"The beach was deserted, except for a lone figure walking along the shoreline. She picked up a shell and held it to her ear, listening for the ocean.",
-"The stars twinkled overhead as the children lay in the grass, pointing out constellations. The night was still, and the world seemed infinite.",
-"The garden was in full bloom, with flowers of every color swaying gently in the breeze. A bumblebee buzzed from one blossom to the next.",
-# Continuing with similar multi-sentence stories
-"The desert stretched out before them, an endless sea of sand. The heat was oppressive, but they knew they had to keep moving.",
-"The market was bustling with vendors selling everything from spices to silk. The air was filled with the scent of exotic foods.",
-"The rain poured down, soaking them to the bone, but they laughed anyway. It was a memory they would cherish forever.",
-"The ship sailed on, its sails full of wind, cutting through the waves. Below deck, the crew prepared for the long journey ahead.",
-"The autumn leaves crunched underfoot as they walked through the park. The crisp air was filled with the scent of pine and earth.",
-"The old library was filled with books of every kind. She wandered through the aisles, running her fingers along the spines.",
-"The clock ticked slowly, marking the minutes as they passed. She stared at the door, waiting for him to return.",
-"The snow fell softly, blanketing the world in white. Everything was still, as though the earth was holding its breath.",
-"The forest floor was covered in a thick layer of fallen leaves, and their footsteps crunched loudly with every step.",
-"The cat stretched lazily in the sun, its tail flicking idly. It watched the birds in the trees with mild interest.",
-"The bakery smelled of fresh bread and cinnamon, making her stomach growl. She couldn't resist buying a warm croissant.",
-"The storm raged outside, but inside the cabin, the fire crackled warmly. They sat together, sipping hot cocoa and watching the flames dance.",
-"The train rumbled over the tracks, and she stared out the window, lost in thought. The scenery blurred as her mind wandered.",
-"The sound of waves crashing against the rocks filled the air. He stood at the edge of the cliff, watching the ocean with awe.",
-"The bustling city square was full of life, with street performers and food stalls at every corner. It was a sensory overload.",
-"The smell of pine filled the air as they hiked through the forest. The trail wound through the trees, leading them deeper into the wilderness.",
-# Continuing for the rest of the stories
-"The airplane soared above the clouds, and the passengers looked out at the endless sky. Below them, the world seemed so small.",
-"The sun set behind the mountains, casting a golden glow over the landscape. They stood in silence, taking in the beauty.",
-"The ancient ruins were overgrown with vines and moss, but they could still see the outlines of old buildings and statues.",
-"The moonlit path led them through the woods, the branches swaying gently in the breeze. The night was calm, but full of mystery.",
-"The ship creaked as it swayed in the harbor. Seagulls called out overhead, their cries echoing in the still air.",
-"The room was filled with the sound of laughter and music as they danced the night away. It was a night to remember.",
-"The train came to a stop, and the passengers began to disembark. She took a deep breath, ready for the adventure ahead.",
-"The firework exploded in a burst of color, lighting up the night sky. The crowd cheered as the next one launched into the air.",
-"In the heart of the old forest, there was a place where magic still thrived. The trees whispered ancient secrets, and the ground pulsed with life.",
-"The sun was shining brightly as the family arrived at the park for a picnic. Laughter filled the air as the children ran to play.",
-"Emily loved to paint. One day, she found a magical brush that brought her paintings to life. Her world was suddenly filled with vibrant creatures.",
-"A young boy named Jake let go of his balloon, watching it soar into the sky. He smiled as it drifted away, imagining where it would go.",
-"The sea was calm, and the boat gently rocked as the fisherman cast his net. He hummed an old tune, feeling at peace with the world.",
-"The library was quiet except for the soft sound of turning pages. A girl sat in the corner, lost in a world of dragons and knights.",
-"The mountain path was steep, but they pressed on, determined to reach the summit. Clouds gathered overhead, and a storm was brewing.",
-"The train sped through the countryside, and the passengers stared out at the passing fields. In one car, a young woman scribbled furiously in a notebook.",
-"The city streets were alive with the buzz of activity. People hurried by, oblivious to the man playing a sad tune on his guitar.",
-"The forest was dark and dense, but they felt a sense of wonder as they ventured deeper. Something ancient watched them from the shadows.",
-"The campfire crackled as stories were shared. The group huddled closer, the cold night air nipping at their faces.",
-"The waterfall roared as it plunged into the river below. The mist hung in the air, creating a rainbow in the afternoon light.",
-"The old house creaked with every step. Dust hung in the air, and the furniture was covered in white sheets, as though it had been abandoned for years.",
-"The beach was deserted, except for a lone figure walking along the shoreline. She picked up a shell and held it to her ear, listening for the ocean.",
-"The stars twinkled overhead as the children lay in the grass, pointing out constellations. The night was still, and the world seemed infinite.",
-"The garden was in full bloom, with flowers of every color swaying gently in the breeze. A bumblebee buzzed from one blossom to the next.",
-# Continuing with similar multi-sentence stories
-"The desert stretched out before them, an endless sea of sand. The heat was oppressive, but they knew they had to keep moving.",
-"The market was bustling with vendors selling everything from spices to silk. The air was filled with the scent of exotic foods.",
-"The rain poured down, soaking them to the bone, but they laughed anyway. It was a memory they would cherish forever.",
-"The ship sailed on, its sails full of wind, cutting through the waves. Below deck, the crew prepared for the long journey ahead.",
-"The autumn leaves crunched underfoot as they walked through the park. The crisp air was filled with the scent of pine and earth.",
-"The old library was filled with books of every kind. She wandered through the aisles, running her fingers along the spines."
-),
-story_id = 1:62
-))
-# Create datasets for testing merging feature help(reframe)
-# Participant level data for testing data merge
-PSE_stories_participant_level <- PSE_stories %>%
-group_by(Participant_ID) %>%
-reframe(stories=paste(Story_Text, collapse = " "))
-# Story level data for testing data merge
-PSE_stories_story_level <- PSE_stories
-# Sentence level data for testing data merge
-PSE_stories_sentence_level <- PSE_stories %>%
-filter()
-#split to sentences help(unnest)
-PSE_stories_sentence_level <- PSE_stories %>%
-mutate(Story_Text = strsplit(Story_Text, "[\\.\\!\\?]\\s+")) %>%
-tidyr::unnest(Story_Text)
-implicit_motive <- text::textClassify(
-model_info = "implicitpower_roberta23_previoussentence_nilsson2024",
-texts = PSE_stories_participant_level$stories,
-show_texts = T
-)
-library(text)
-textrpp_initialize()
-.rs.restartR()
-library(text)
-textrpp_initialize()
-implicit_motive <- text::textClassify(
-model_info = "implicitpower_roberta23_previoussentence_nilsson2024",
-texts = PSE_stories_participant_level$stories,
-show_texts = T
-)
-implicit_motive
-implicit_motive <- text::textClassify(
-model_info = "implicitachievment_roberta23_previoussentence_nilsson2024",
-texts = PSE_stories_participant_level$stories,
-show_texts = T
-)
-implicit_motive <- text::textClassify(
-model_info = "implicitaffiliation_roberta23_previoussentence_nilsson2024",
-texts = PSE_stories_participant_level$stories,
-show_texts = T
-)
-implicit_motive
-implicit_motive <- text::textClassify(
-model_info = "implicitachievement_roberta23_previoussentence_nilsson2024",
-texts = PSE_stories_participant_level$stories,
-show_texts = T
-)
-implicit_motive
-implicit_achievement <- text::textClassify(
-model_info = "implicitachievement_roberta23_previoussentence_nilsson2024",
-texts = PSE_stories_participant_level$stories,
-show_texts = T
-)
-schone_training <- readRDS("/Users/oscarkjell/Desktop/schone_training_150424.rds")
-predictions_powerrob_2 <- textPredict(
-texts = schone_training$text_english[1:50],
-model_info = "implicitpower_roberta23_nilsson2024",
-story_id = schone_training$story_id_num[1:50],
-dataset_to_merge_predictions = schone_training[1:50,],
-show_texts = T)
-predictions_powerrob_2
-predictions_powerrob_1 <- textPredict(
-texts = schone_training$text_english[1:50],
-model_info = "implicitpower_roberta23_nilsson2024")
-predictions_powerrob_1
-predictions_powerrob_2$person_predictions$person_class_no_wc_correction
-predictions_powerrob_2$person_predictions
-predictions_powerrob_2$person_predictions$person_class_no_wc_correction
-predictions_powerrob_4 <- textPredict(
-texts = schone_training$text_english[1:50],
-model_info = "implicitpower_roberta23_nilsson2024",
-story_id = schone_training$story_id_num[1:50],
-participant_id = schone_training$participant_id[1:50],
-dataset_to_merge_predictions = schone_training[1:50,])
-predictions_powerrob_4
-implicit_power <- text::textClassify(
-model_info = "implicitpower_roberta23_previoussentence_nilsson2024",
-texts = PSE_stories_participant_level$stories,
-show_texts = T
-)
-devtools::check_win_release() # If you’re on linux or the mac, use devtools::build_win() to check on windows.
+clean_up_tokenization_spaces = FALSE,
+prefix = "",
+handle_long_generation = "hole",
+set_seed = 22L
+)
+expect_that(generated_text3$x_generated, is_a("character"))
+library(testthat)
+generated_text3 <- text::textGeneration(
+x = "The meaning of life is",
+model = "gpt2",
+device = "cpu",
+tokenizer_parallelism = FALSE,
+max_new_tokens = 2,
+logging_level = "warning",
+force_return_results = FALSE,
+return_tensors = TRUE,
+return_full_text = FALSE,
+clean_up_tokenization_spaces = FALSE,
+prefix = "",
+handle_long_generation = "hole",
+set_seed = 22L
+)
+expect_that(generated_text3$x_generated, is_a("character"))
+text::textGeneration(
+x = "The meaning of life is",
+model = "gpt2",
+device = "cpu",
+tokenizer_parallelism = FALSE,
+max_new_tokens = 2,
+logging_level = "warning",
+force_return_results = FALSE,
+return_tensors = TRUE,
+return_full_text = FALSE,
+clean_up_tokenization_spaces = FALSE,
+prefix = "",
+handle_long_generation = "hole",
+set_seed = 22L
+)
+generated_text2 <- text::textGeneration(
+x = "The meaning of life is",
+model = "gpt2",
+device = "cpu",
+tokenizer_parallelism = FALSE,
+max_new_tokens = 2,
+logging_level = "warning",
+force_return_results = FALSE,
+return_tensors = FALSE,
+return_full_text = TRUE,
+clean_up_tokenization_spaces = FALSE,
+prefix = "",
+handle_long_generation = "hole",
+set_seed = 22L
+)
+expect_that(generated_text2$x_generated, is_a("character"))
+text::textGeneration(
+x = "The meaning of life is",
+model = "gpt2",
+device = "cpu",
+tokenizer_parallelism = FALSE,
+max_new_tokens = 2,
+logging_level = "warning",
+force_return_results = FALSE,
+return_tensors = FALSE,
+return_full_text = TRUE,
+clean_up_tokenization_spaces = FALSE,
+prefix = "",
+handle_long_generation = "hole",
+set_seed = 22L
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2,
+max_length = 1,
+return_full_text = FALSE,
+set_seed = 111L
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2,
+max_length = 1L,
+return_full_text = FALSE,
+set_seed = 111L
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = 1L,
+return_full_text = FALSE,
+set_seed = 111
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = 1,
+return_full_text = FALSE,
+set_seed = 111
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = 1,
+return_full_text = FALSE,
+set_seed = 111
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = 1,
+return_full_text = FALSE,
+set_seed = 111L
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = 1,
+return_full_text = FALSE,
+set_seed = 111L
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = 1,
+return_full_text = FALSE,
+set_seed = 111L
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = 1,
+return_full_text = FALSE,
+set_seed = 111
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = 1,
+return_full_text = FALSE,
+set_seed = 111
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = NULL,
+return_full_text = FALSE,
+set_seed = 111
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = NULL,
+return_full_text = FALSE,
+set_seed = 111
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = NULL,
+return_full_text = FALSE,
+set_seed = 111
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = NULL,
+return_full_text = FALSE,
+set_seed = 111
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = NULL,
+return_full_text = FALSE,
+set_seed = 111
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = 1,
+return_full_text = FALSE,
+set_seed = 111
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = 1,
+return_full_text = FALSE,
+set_seed = 111
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = 1,
+return_full_text = FALSE,
+set_seed = 111
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = 1L,
+return_full_text = FALSE,
+set_seed = 111
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = 1L,
+return_full_text = FALSE,
+set_seed = 111
+)
+generated_text2 <- text::textGeneration(
+x = "The meaning of life is",
+model = "gpt2",
+device = "cpu",
+tokenizer_parallelism = FALSE,
+max_new_tokens = 2,
+logging_level = "warning",
+force_return_results = FALSE,
+return_tensors = FALSE,
+return_full_text = TRUE,
+clean_up_tokenization_spaces = FALSE,
+prefix = "",
+handle_long_generation = "hole",
+set_seed = 22
+)
+generated_text2
+expect_that(generated_text2$x_generated, is_a("character"))
+generated_text2 <- text::textGeneration(
+x = "The meaning of life is",
+model = "gpt2",
+device = "cpu",
+tokenizer_parallelism = FALSE,
+max_new_tokens = 2,
+logging_level = "warning",
+force_return_results = FALSE,
+return_tensors = FALSE,
+return_full_text = TRUE,
+clean_up_tokenization_spaces = FALSE,
+prefix = "",
+handle_long_generation = "hole",
+set_seed = 22
+)
+generated_text2
+expect_that(generated_text2$x_generated, is_a("character"))
+generated_text3 <- text::textGeneration(
+x = "The meaning of life is",
+model = "gpt2",
+device = "cpu",
+tokenizer_parallelism = FALSE,
+max_length = 2,
+logging_level = "warning",
+force_return_results = FALSE,
+return_tensors = FALSE,
+return_full_text = TRUE,
+clean_up_tokenization_spaces = FALSE,
+prefix = "",
+handle_long_generation = "hole",
+set_seed = 22L
+)
+generated_text3 <- text::textGeneration(
+x = "The meaning of life is",
+model = "gpt2",
+device = "cpu",
+tokenizer_parallelism = FALSE,
+max_length = 20,
+logging_level = "warning",
+force_return_results = FALSE,
+return_tensors = FALSE,
+return_full_text = TRUE,
+clean_up_tokenization_spaces = FALSE,
+prefix = "",
+handle_long_generation = "hole",
+set_seed = 22L
+)
+generated_text3
+generated_text3 <- text::textGeneration(
+x = "The meaning of life is",
+model = "gpt2",
+device = "cpu",
+tokenizer_parallelism = FALSE,
+max_length = 20,
+logging_level = "warning",
+force_return_results = FALSE,
+return_tensors = FALSE,
+return_full_text = TRUE,
+clean_up_tokenization_spaces = FALSE,
+prefix = "",
+handle_long_generation = "hole",
+set_seed = 22L
+)
+expect_that(generated_text3$x_generated, is_a("character"))
+text::textGeneration(
+x = "The meaning of life is",
+model = "gpt2",
+device = "cpu",
+tokenizer_parallelism = FALSE,
+max_length = 3L,
+max_new_tokens = NULL,
+logging_level = "warning",
+force_return_results = FALSE,
+return_tensors = FALSE,
+return_full_text = TRUE,
+clean_up_tokenization_spaces = FALSE,
+prefix = "",
+handle_long_generation = "hole",
+set_seed = 22L
+)
+text::textGeneration(
+x = "The meaning of life is",
+model = "gpt2",
+device = "cpu",
+tokenizer_parallelism = FALSE,
+max_length = 3L,
+max_new_tokens = NULL,
+logging_level = "warning",
+force_return_results = FALSE,
+return_tensors = FALSE,
+return_full_text = TRUE,
+clean_up_tokenization_spaces = FALSE,
+prefix = "",
+handle_long_generation = "hole",
+set_seed = 22L
+) |> expect_error()
+text::textGeneration(
+x = "The meaning of life is",
+model = "gpt2",
+device = "cpu",
+tokenizer_parallelism = FALSE,
+max_length = 3L,
+max_new_tokens = NULL,
+logging_level = "warning",
+force_return_results = FALSE,
+return_tensors = FALSE,
+return_full_text = TRUE,
+clean_up_tokenization_spaces = FALSE,
+prefix = "",
+handle_long_generation = "hole",
+set_seed = 22L
+) |> expect_error()
+text::textGeneration(
+x = "The meaning of life is",
+model = "gpt2",
+device = "cpu",
+tokenizer_parallelism = FALSE,
+max_length = 3L,
+max_new_tokens = NULL,
+logging_level = "warning",
+force_return_results = FALSE,
+return_tensors = FALSE,
+return_full_text = TRUE,
+clean_up_tokenization_spaces = FALSE,
+prefix = "",
+handle_long_generation = "hole",
+set_seed = 22L
+)
+text::textGeneration(
+x = "The meaning of life is",
+model = "gpt2",
+device = "cpu",
+tokenizer_parallelism = FALSE,
+max_length = 3L,
+max_new_tokens = NULL,
+logging_level = "warning",
+force_return_results = FALSE,
+return_tensors = FALSE,
+return_full_text = TRUE,
+clean_up_tokenization_spaces = FALSE,
+prefix = "",
+handle_long_generation = "hole",
+set_seed = 22L
+) |> expect_error(regexp = 'ValueError\: Input length')
+text::textGeneration(
+x = "The meaning of life is",
+model = "gpt2",
+device = "cpu",
+tokenizer_parallelism = FALSE,
+max_length = 3L,
+max_new_tokens = NULL,
+logging_level = "warning",
+force_return_results = FALSE,
+return_tensors = FALSE,
+return_full_text = TRUE,
+clean_up_tokenization_spaces = FALSE,
+prefix = "",
+handle_long_generation = "hole",
+set_seed = 22L
+) |> expect_error(regexp = 'ValueError\\: Input length')
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2,
+set_seed = 111L
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2,
+set_seed = 111L
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2,
+set_seed = 111L
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+set_seed = 111L
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+set_seed = 111L
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = 1,
+return_full_text = FALSE,
+set_seed = 111
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = 1,
+return_full_text = FALSE,
+set_seed = 111
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = 1L,
+return_full_text = FALSE,
+set_seed = 111
+)
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 2L,
+max_length = 1L,
+return_full_text = FALSE,
+set_seed = 111
+)
+tibble::`%>`
+tibble::`%>%`
+text::textGeneration(
+x = "The meaning of life is",
+model = "gpt2",
+device = "cpu",
+tokenizer_parallelism = FALSE,
+max_length = 3L,
+max_new_tokens = NULL,
+logging_level = "warning",
+force_return_results = FALSE,
+return_tensors = FALSE,
+return_full_text = TRUE,
+clean_up_tokenization_spaces = FALSE,
+prefix = "",
+handle_long_generation = "hole",
+set_seed = 22L
+) %>% expect_error(regexp = 'ValueError\\: Input length')
+text::textGeneration(
+x = "The meaning of life is",
+model = "gpt2",
+device = "cpu",
+tokenizer_parallelism = FALSE,
+max_length = 3L,
+max_new_tokens = NULL,
+logging_level = "warning",
+force_return_results = FALSE,
+return_tensors = FALSE,
+return_full_text = TRUE,
+clean_up_tokenization_spaces = FALSE,
+prefix = "",
+handle_long_generation = "hole",
+set_seed = 22L
+) %>% expect_error(regexp = 'ValueError\\: Input length')
+textGeneration(
+'A long text that exceed the `max_length` parameter value.',
+model = 'gpt2',
+max_new_tokens = 0L,
+max_length = 1L,
+return_full_text = FALSE,
+set_seed = 111
+)
+import numpy as np
+reticulate::repl_python()
diff --git a/.gitignore b/.gitignore
index 8b1b45f..67b4169 100644
--- a/.gitignore
+++ b/.gitignore
@@ -4,6 +4,8 @@
 .Ruserdata
 docs
 .DS_Store
+dev
+.Rhistory
 # Compiled source #
 ###################
 *.a
diff --git a/R/5_2_textGeneration.R b/R/5_2_textGeneration.R
index 7a357d8..ea126e9 100644
--- a/R/5_2_textGeneration.R
+++ b/R/5_2_textGeneration.R
@@ -21,6 +21,10 @@ set_seed = 22L
 #' autoregressive language modeling objective, which includes the uni-directional models (e.g., gpt2).
 #' @param device (string)  Device to use: 'cpu', 'gpu', or 'gpu:k' where k is a specific device number
 #' @param tokenizer_parallelism (boolean)  If TRUE this will turn on tokenizer parallelism.
+#' @param max_length (Integer)  The maximum length the generated tokens can have. Corresponds to the length of the input prompt + `max_new_tokens`. Its effect is overridden by `max_new_tokens`, if also set. Defaults to NULL.
+#' @param max_new_tokens (Integer)  The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt. The default value is 20.
+#' @param min_length (Integer)  The minimum length of the sequence to be generated. Corresponds to the length of the input prompt + `min_new_tokens`. Its effect is overridden by `min_new_tokens`, if also set. The default value is 0.
+#' @param min_new_tokens (Integer)  The minimum numbers of tokens to generate, ignoring the number of tokens in the prompt. Default is NULL.
 #' @param logging_level (string)  Set the logging level.
 #' Options (ordered from less logging to more logging): critical, error, warning, info, debug
 #' @param force_return_results (boolean)  Stop returning some incorrectly formatted/structured results.
@@ -56,6 +60,10 @@ textGeneration <- function(x,
                            model = "gpt2",
                            device = "cpu",
                            tokenizer_parallelism = FALSE,
+                           max_length = NULL,
+                           max_new_tokens = 20,
+                           min_length = 0,
+                           min_new_tokens = NULL,
                            logging_level = "warning",
                            force_return_results = FALSE,
                            return_tensors = FALSE,
@@ -72,6 +80,11 @@ textGeneration <- function(x,
     mustWork = TRUE
   ))
 
+  # Convert integer arguments explicitly for Python
+  ## `max_length` and `max_length` are converted inside the Python function
+  if (!is.null(set_seed)) set_seed <- as.integer(set_seed)
+
+
   # Select all character variables and make them UTF-8 coded (e.g., BERT wants it that way).
   data_character_variables <- select_character_v_utf8(x)
 
@@ -87,6 +100,8 @@ textGeneration <- function(x,
       model = model,
       device = device,
       tokenizer_parallelism = tokenizer_parallelism,
+      max_length = max_length,
+      max_new_tokens = max_new_tokens,
       logging_level = logging_level,
       force_return_results = force_return_results,
       return_tensors = return_tensors,
diff --git a/inst/python/huggingface_Interface3.py b/inst/python/huggingface_Interface3.py
index 9f0151d..3ea58ad 100644
--- a/inst/python/huggingface_Interface3.py
+++ b/inst/python/huggingface_Interface3.py
@@ -215,14 +215,14 @@ def get_model(model, tokenizer_only=False, config_only=False, hg_gated=False, hg
     elif tokenizer_only:
         # Do not know how to fix this. Some decoder-only files do not have pad_token.
         if tokenizer.pad_token is None:
-            print("The language model entered might has issues since the model does not provide the padding_token.")
+            print("The language model entered might have issues since the model does not provide the padding_token.")
             print("Consider use BERT-like models instead if meeting errors.")
         #    tokenizer.pad_token = tokenizer.eos_token
         #    tokenizer.pad_token_id = tokenizer.eos_token_id
         return tokenizer
     else:
         if tokenizer.pad_token is None:
-            print("The language model entered might has issues since the model does not provide the padding_token.")
+            print("The language model entered might have issues since the model does not provide the padding_token.")
             print("Consider use BERT-like models instead if meeting errors.")    
         #    tokenizer.pad_token = tokenizer.eos_token
         #    tokenizer.pad_token_id = tokenizer.eos_token_id        
@@ -334,10 +334,23 @@ def hgTransformerGetPipeline(text_strings,
     return task_scores
 
 
+# Convert floats to integers or propagate None
+def _as_integer(x):
+    if isinstance(x, int) or isinstance(x, np.integer):
+        return x
+    elif isinstance(x, float) or isinstance(x, np.floating):
+        return int(x)
+    else:
+        return None
+
 def hgTransformerGetTextGeneration(text_strings,
                             model = '',
                             device = 'cpu',
                             tokenizer_parallelism = False,
+                            max_length = None,
+                            max_new_tokens = 20,
+                            min_length = 0,
+                            min_new_tokens = None,
                             logging_level = 'warning',
                             force_return_results = False,
                             set_seed = None,
@@ -347,6 +360,21 @@ def hgTransformerGetTextGeneration(text_strings,
                             clean_up_tokenization_spaces = False,
                             prefix = '', 
                             handle_long_generation = None):
+    # Prepare kwargs
+    if max_new_tokens is not None and max_new_tokens <= 0:
+        print(f"Warning: `max_new_tokens` must be greater than 0, but is {max_new_tokens}")
+        print( "         Using default value…")
+        max_new_tokens = None
+    generation_kwargs = {
+        'max_length': _as_integer(max_length),
+        'min_length': _as_integer(min_length),
+        'min_new_tokens': _as_integer(min_new_tokens)
+    }
+    # `max_new_tokens` should not be explicitly None
+    max_new_tokens = _as_integer(max_new_tokens)
+    if max_new_tokens is not None:
+        generation_kwargs['max_new_tokens'] = max_new_tokens
+    
     if return_tensors:
         if return_full_text:
             print("Warning: you set return_tensors and return_text (or return_full_text)")
@@ -363,7 +391,8 @@ def hgTransformerGetTextGeneration(text_strings,
                             return_tensors = return_tensors, 
                             clean_up_tokenization_spaces = clean_up_tokenization_spaces, 
                             prefix = prefix,
-                            handle_long_generation = handle_long_generation)
+                            handle_long_generation = handle_long_generation,
+                            **generation_kwargs)
     else:
         generated_texts = hgTransformerGetPipeline(text_strings = text_strings,
                             task = 'text-generation',
@@ -378,7 +407,8 @@ def hgTransformerGetTextGeneration(text_strings,
                             return_full_text = return_full_text, 
                             clean_up_tokenization_spaces = clean_up_tokenization_spaces, 
                             prefix = prefix,
-                            handle_long_generation = handle_long_generation)
+                            handle_long_generation = handle_long_generation,
+                            **generation_kwargs)
     return generated_texts
 
 def hgTransformerGetNER(text_strings,
diff --git a/tests/testthat/test_5_Tasks.R b/tests/testthat/test_5_Tasks.R
index 858ed40..d9bc5f9 100644
--- a/tests/testthat/test_5_Tasks.R
+++ b/tests/testthat/test_5_Tasks.R
@@ -18,7 +18,7 @@ test_that("textClassify tests", {
     function_to_apply = "none"
   )
   expect_equal(sen1$score_x, 4.67502, tolerance = 0.001)
-  textModelsRemove("distilbert-base-uncased-finetuned-sst-2-english")
+  # textModelsRemove("distilbert-base-uncased-finetuned-sst-2-english")
   #
 })
 
@@ -46,9 +46,129 @@ test_that("textGeneration test", {
   # the child of a dead person. It is a name, a life, and not as the son, daughter")
   expect_that(generated_text$x_generated, is_a("character"))
 
-  # Return token IDs
+  # Use `max_new_tokens` and convert set_seed to integer
   print("textGeneration_2")
   generated_text2 <- text::textGeneration(
+    x = "The meaning of life is",
+    model = "gpt2",
+    device = "cpu",
+    tokenizer_parallelism = FALSE,
+    max_new_tokens = 2,
+    logging_level = "warning",
+    force_return_results = FALSE,
+    return_tensors = FALSE,
+    return_full_text = TRUE,
+    clean_up_tokenization_spaces = FALSE,
+    prefix = "",
+    handle_long_generation = "hole",
+    set_seed = 22
+  )
+  expect_that(generated_text2$x_generated, is_a("character"))
+
+  # Use `max_length`
+  print("textGeneration_3")
+  generated_text3 <- text::textGeneration(
+    x = "The meaning of life is",
+    model = "gpt2",
+    device = "cpu",
+    tokenizer_parallelism = FALSE,
+    max_length = 20,
+    logging_level = "warning",
+    force_return_results = FALSE,
+    return_tensors = FALSE,
+    return_full_text = TRUE,
+    clean_up_tokenization_spaces = FALSE,
+    prefix = "",
+    handle_long_generation = "hole",
+    set_seed = 22L
+  )
+  expect_that(generated_text3$x_generated, is_a("character"))
+
+  # Use to small a `max_length` without setting `max_new_tokens`
+  print("textGeneration_4")
+  text::textGeneration(
+    x = "The meaning of life is",
+    model = "gpt2",
+    device = "cpu",
+    tokenizer_parallelism = FALSE,
+    max_length = 3L,
+    max_new_tokens = NULL,
+    logging_level = "warning",
+    force_return_results = FALSE,
+    return_tensors = FALSE,
+    return_full_text = TRUE,
+    clean_up_tokenization_spaces = FALSE,
+    prefix = "",
+    handle_long_generation = "hole",
+    set_seed = 22L
+  ) %>% expect_error(regexp = 'ValueError\\: Input length')
+
+  # Use `min_length`
+  print("textGeneration_5")
+  generated_text5 <- text::textGeneration(
+    x = "The meaning of life is",
+    model = "gpt2",
+    device = "cpu",
+    tokenizer_parallelism = FALSE,
+    max_length = NULL,
+    min_length = 20L,
+    max_new_tokens = NULL,
+    logging_level = "warning",
+    force_return_results = FALSE,
+    return_tensors = FALSE,
+    return_full_text = TRUE,
+    clean_up_tokenization_spaces = FALSE,
+    prefix = "",
+    handle_long_generation = "hole",
+    set_seed = 22L
+  )
+  expect_that(generated_text5$x_generated, is_a("character"))
+
+  # Use `min_new_tokens`
+  print("textGeneration_6")
+  generated_text6 <- text::textGeneration(
+    x = "The meaning of life is",
+    model = "gpt2",
+    device = "cpu",
+    tokenizer_parallelism = FALSE,
+    min_new_tokens = 10,
+    max_new_tokens = NULL,
+    logging_level = "warning",
+    force_return_results = FALSE,
+    return_tensors = FALSE,
+    return_full_text = TRUE,
+    clean_up_tokenization_spaces = FALSE,
+    prefix = "",
+    handle_long_generation = "hole",
+    set_seed = 22L
+  )
+  expect_that(generated_text6$x_generated, is_a("character"))
+
+  # All length parameters set to NULL
+  print("textGeneration_7")
+  generated_text7 <- text::textGeneration(
+    x = "The meaning of life is",
+    model = "gpt2",
+    device = "cpu",
+    tokenizer_parallelism = FALSE,
+    max_length = NULL,
+    min_length = NULL,
+    max_new_tokens = NULL,
+    min_new_tokens = NULL,
+    logging_level = "warning",
+    force_return_results = FALSE,
+    return_tensors = FALSE,
+    return_full_text = TRUE,
+    clean_up_tokenization_spaces = FALSE,
+    prefix = "",
+    handle_long_generation = "hole",
+    set_seed = 22L
+  )
+  expect_that(generated_text7$x_generated, is_a("character"))
+
+  # Return token IDs
+  print("textGeneration_8")
+  generated_text8 <- text::textGeneration(
     x = "The meaning of life is",
     model = "gpt2",
     device = "cpu",
@@ -62,9 +182,9 @@ test_that("textGeneration test", {
     handle_long_generation = "hole",
     set_seed = 22L
   )
-  textModelsRemove("gpt2")
-  expect_equal(generated_text2$generated_token_ids[1], 464)
-  expect_that(generated_text2$generated_token_ids[1], is_a("integer"))
+  # textModelsRemove("gpt2")
+  expect_equal(generated_text8$generated_token_ids[1], 464)
+  expect_that(generated_text8$generated_token_ids[1], is_a("integer"))
 })
 
 
@@ -87,7 +207,7 @@ test_that("textNER test", {
   )
   ner_example2
   expect_equal(ner_example2$satisfactiontexts_NER$score[2], 0.976, tolerance = 0.01)
-  textModelsRemove("dslim/bert-base-NER")
+  # textModelsRemove("dslim/bert-base-NER")
 })
 
 test_that("textSum test", {
@@ -132,7 +252,7 @@ test_that("textZeroShot test", {
   )
 
   testthat::expect_equal(ZeroShot_example$scores_x_1[1], 0.3341856, tolerance = 0.00001)
-  textModelsRemove("okho0653/distilbert-base-uncased-zero-shot-sentiment-model")
+  # textModelsRemove("okho0653/distilbert-base-uncased-zero-shot-sentiment-model")
 })
 
 test_that("textTranslate test", {
@@ -153,5 +273,5 @@ test_that("textTranslate test", {
     translation_example$en_to_fr_satisfactiontexts[1],
     "Je ne suis pas satisfait de ma vie, je suis reconnaissante de ce que j'ai et de ce que je suis, car la situation peut toujours être pire. Je veux une carrière et un diplôme, je veux perdre de poids et je n'ai pas encore atteint ces objectifs."
   )
-  textModelsRemove("t5-small")
+  # textModelsRemove("t5-small")
 })
