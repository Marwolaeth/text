I want to make a pull request in the `text` r package that uses Hugging Face transformers language models, natural language processing and machine learning methods to examine text and numerical variables. This would be my second PR if I make it.

Here are the diffs:


```diff
diff --git a/inst/python/huggingface_Interface3.py b/inst/python/huggingface_Interface3.py
index 9f0151d..3ea58ad 100644
--- a/inst/python/huggingface_Interface3.py
+++ b/inst/python/huggingface_Interface3.py
@@ -215,14 +215,14 @@ def get_model(model, tokenizer_only=False, config_only=False, hg_gated=False, hg
     elif tokenizer_only:
         # Do not know how to fix this. Some decoder-only files do not have pad_token.
         if tokenizer.pad_token is None:
-            print("The language model entered might has issues since the model does not provide the padding_token.")
+            print("The language model entered might have issues since the model does not provide the padding_token.")
             print("Consider use BERT-like models instead if meeting errors.")
         #    tokenizer.pad_token = tokenizer.eos_token
         #    tokenizer.pad_token_id = tokenizer.eos_token_id
         return tokenizer
     else:
         if tokenizer.pad_token is None:
-            print("The language model entered might has issues since the model does not provide the padding_token.")
+            print("The language model entered might have issues since the model does not provide the padding_token.")
             print("Consider use BERT-like models instead if meeting errors.")    
         #    tokenizer.pad_token = tokenizer.eos_token
         #    tokenizer.pad_token_id = tokenizer.eos_token_id        
@@ -334,10 +334,23 @@ def hgTransformerGetPipeline(text_strings,
     return task_scores
 
 
+# Convert floats to integers or propagate None
+def _as_integer(x):
+    if isinstance(x, int) or isinstance(x, np.integer):
+        return x
+    elif isinstance(x, float) or isinstance(x, np.floating):
+        return int(x)
+    else:
+        return None
+
 def hgTransformerGetTextGeneration(text_strings,
                             model = '',
                             device = 'cpu',
                             tokenizer_parallelism = False,
+                            max_length = None,
+                            max_new_tokens = 20,
+                            min_length = 0,
+                            min_new_tokens = None,
                             logging_level = 'warning',
                             force_return_results = False,
                             set_seed = None,
@@ -347,6 +360,21 @@ def hgTransformerGetTextGeneration(text_strings,
                             clean_up_tokenization_spaces = False,
                             prefix = '', 
                             handle_long_generation = None):
+    # Prepare kwargs
+    if max_new_tokens is not None and max_new_tokens <= 0:
+        print(f"Warning: `max_new_tokens` must be greater than 0, but is {max_new_tokens}")
+        print( "         Using default valueâ€¦")
+        max_new_tokens = None
+    generation_kwargs = {
+        'max_length': _as_integer(max_length),
+        'min_length': _as_integer(min_length),
+        'min_new_tokens': _as_integer(min_new_tokens)
+    }
+    # `max_new_tokens` should not be explicitly None
+    max_new_tokens = _as_integer(max_new_tokens)
+    if max_new_tokens is not None:
+        generation_kwargs['max_new_tokens'] = max_new_tokens
+    
     if return_tensors:
         if return_full_text:
             print("Warning: you set return_tensors and return_text (or return_full_text)")
@@ -363,7 +391,8 @@ def hgTransformerGetTextGeneration(text_strings,
                             return_tensors = return_tensors, 
                             clean_up_tokenization_spaces = clean_up_tokenization_spaces, 
                             prefix = prefix,
-                            handle_long_generation = handle_long_generation)
+                            handle_long_generation = handle_long_generation,
+                            **generation_kwargs)
     else:
         generated_texts = hgTransformerGetPipeline(text_strings = text_strings,
                             task = 'text-generation',
@@ -378,7 +407,8 @@ def hgTransformerGetTextGeneration(text_strings,
                             return_full_text = return_full_text, 
                             clean_up_tokenization_spaces = clean_up_tokenization_spaces, 
                             prefix = prefix,
-                            handle_long_generation = handle_long_generation)
+                            handle_long_generation = handle_long_generation,
+                            **generation_kwargs)
     return generated_texts
 
 def hgTransformerGetNER(text_strings,

diff --git a/R/5_2_textGeneration.R b/R/5_2_textGeneration.R
index 7a357d8..ea126e9 100644
--- a/R/5_2_textGeneration.R
+++ b/R/5_2_textGeneration.R
@@ -21,6 +21,10 @@ set_seed = 22L
 #' autoregressive language modeling objective, which includes the uni-directional models (e.g., gpt2).
 #' @param device (string)  Device to use: 'cpu', 'gpu', or 'gpu:k' where k is a specific device number
 #' @param tokenizer_parallelism (boolean)  If TRUE this will turn on tokenizer parallelism.
+#' @param max_length (Integer)  The maximum length the generated tokens can have. Corresponds to the length of the input prompt + `max_new_tokens`. Its effect is overridden by `max_new_tokens`, if also set. Defaults to NULL.
+#' @param max_new_tokens (Integer)  The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt. The default value is 20.
+#' @param min_length (Integer)  The minimum length of the sequence to be generated. Corresponds to the length of the input prompt + `min_new_tokens`. Its effect is overridden by `min_new_tokens`, if also set. The default value is 0.
+#' @param min_new_tokens (Integer)  The minimum numbers of tokens to generate, ignoring the number of tokens in the prompt. Default is NULL.
 #' @param logging_level (string)  Set the logging level.
 #' Options (ordered from less logging to more logging): critical, error, warning, info, debug
 #' @param force_return_results (boolean)  Stop returning some incorrectly formatted/structured results.
@@ -56,6 +60,10 @@ textGeneration <- function(x,
                            model = "gpt2",
                            device = "cpu",
                            tokenizer_parallelism = FALSE,
+                           max_length = NULL,
+                           max_new_tokens = 20,
+                           min_length = 0,
+                           min_new_tokens = NULL,
                            logging_level = "warning",
                            force_return_results = FALSE,
                            return_tensors = FALSE,
@@ -72,6 +80,11 @@ textGeneration <- function(x,
     mustWork = TRUE
   ))
 
+  # Convert integer arguments explicitly for Python
+  ## `max_length` and `max_length` are converted inside the Python function
+  if (!is.null(set_seed)) set_seed <- as.integer(set_seed)
+
+
   # Select all character variables and make them UTF-8 coded (e.g., BERT wants it that way).
   data_character_variables <- select_character_v_utf8(x)
 
@@ -87,6 +100,8 @@ textGeneration <- function(x,
       model = model,
       device = device,
       tokenizer_parallelism = tokenizer_parallelism,
+      max_length = max_length,
+      max_new_tokens = max_new_tokens,
       logging_level = logging_level,
       force_return_results = force_return_results,
       return_tensors = return_tensors,

diff --git a/tests/testthat/test_5_Tasks.R b/tests/testthat/test_5_Tasks.R
index 858ed40..d9bc5f9 100644
--- a/tests/testthat/test_5_Tasks.R
+++ b/tests/testthat/test_5_Tasks.R
@@ -18,7 +18,7 @@ test_that("textClassify tests", {
     function_to_apply = "none"
   )
   expect_equal(sen1$score_x, 4.67502, tolerance = 0.001)
-  textModelsRemove("distilbert-base-uncased-finetuned-sst-2-english")
+  # textModelsRemove("distilbert-base-uncased-finetuned-sst-2-english")
   #
 })
 
@@ -46,9 +46,129 @@ test_that("textGeneration test", {
   # the child of a dead person. It is a name, a life, and not as the son, daughter")
   expect_that(generated_text$x_generated, is_a("character"))
 
-  # Return token IDs
+  # Use `max_new_tokens` and convert set_seed to integer
   print("textGeneration_2")
   generated_text2 <- text::textGeneration(
+    x = "The meaning of life is",
+    model = "gpt2",
+    device = "cpu",
+    tokenizer_parallelism = FALSE,
+    max_new_tokens = 2,
+    logging_level = "warning",
+    force_return_results = FALSE,
+    return_tensors = FALSE,
+    return_full_text = TRUE,
+    clean_up_tokenization_spaces = FALSE,
+    prefix = "",
+    handle_long_generation = "hole",
+    set_seed = 22
+  )
+  expect_that(generated_text2$x_generated, is_a("character"))
+
+  # Use `max_length`
+  print("textGeneration_3")
+  generated_text3 <- text::textGeneration(
+    x = "The meaning of life is",
+    model = "gpt2",
+    device = "cpu",
+    tokenizer_parallelism = FALSE,
+    max_length = 20,
+    logging_level = "warning",
+    force_return_results = FALSE,
+    return_tensors = FALSE,
+    return_full_text = TRUE,
+    clean_up_tokenization_spaces = FALSE,
+    prefix = "",
+    handle_long_generation = "hole",
+    set_seed = 22L
+  )
+  expect_that(generated_text3$x_generated, is_a("character"))
+
+  # Use to small a `max_length` without setting `max_new_tokens`
+  print("textGeneration_4")
+  text::textGeneration(
+    x = "The meaning of life is",
+    model = "gpt2",
+    device = "cpu",
+    tokenizer_parallelism = FALSE,
+    max_length = 3L,
+    max_new_tokens = NULL,
+    logging_level = "warning",
+    force_return_results = FALSE,
+    return_tensors = FALSE,
+    return_full_text = TRUE,
+    clean_up_tokenization_spaces = FALSE,
+    prefix = "",
+    handle_long_generation = "hole",
+    set_seed = 22L
+  ) %>% expect_error(regexp = 'ValueError\\: Input length')
+
+  # Use `min_length`
+  print("textGeneration_5")
+  generated_text5 <- text::textGeneration(
+    x = "The meaning of life is",
+    model = "gpt2",
+    device = "cpu",
+    tokenizer_parallelism = FALSE,
+    max_length = NULL,
+    min_length = 20L,
+    max_new_tokens = NULL,
+    logging_level = "warning",
+    force_return_results = FALSE,
+    return_tensors = FALSE,
+    return_full_text = TRUE,
+    clean_up_tokenization_spaces = FALSE,
+    prefix = "",
+    handle_long_generation = "hole",
+    set_seed = 22L
+  )
+  expect_that(generated_text5$x_generated, is_a("character"))
+
+  # Use `min_new_tokens`
+  print("textGeneration_6")
+  generated_text6 <- text::textGeneration(
+    x = "The meaning of life is",
+    model = "gpt2",
+    device = "cpu",
+    tokenizer_parallelism = FALSE,
+    min_new_tokens = 10,
+    max_new_tokens = NULL,
+    logging_level = "warning",
+    force_return_results = FALSE,
+    return_tensors = FALSE,
+    return_full_text = TRUE,
+    clean_up_tokenization_spaces = FALSE,
+    prefix = "",
+    handle_long_generation = "hole",
+    set_seed = 22L
+  )
+  expect_that(generated_text6$x_generated, is_a("character"))
+
+  # All length parameters set to NULL
+  print("textGeneration_7")
+  generated_text7 <- text::textGeneration(
+    x = "The meaning of life is",
+    model = "gpt2",
+    device = "cpu",
+    tokenizer_parallelism = FALSE,
+    max_length = NULL,
+    min_length = NULL,
+    max_new_tokens = NULL,
+    min_new_tokens = NULL,
+    logging_level = "warning",
+    force_return_results = FALSE,
+    return_tensors = FALSE,
+    return_full_text = TRUE,
+    clean_up_tokenization_spaces = FALSE,
+    prefix = "",
+    handle_long_generation = "hole",
+    set_seed = 22L
+  )
+  expect_that(generated_text7$x_generated, is_a("character"))
+
+  # Return token IDs
+  print("textGeneration_8")
+  generated_text8 <- text::textGeneration(
     x = "The meaning of life is",
     model = "gpt2",
     device = "cpu",
@@ -62,9 +182,9 @@ test_that("textGeneration test", {
     handle_long_generation = "hole",
     set_seed = 22L
   )
-  textModelsRemove("gpt2")
-  expect_equal(generated_text2$generated_token_ids[1], 464)
-  expect_that(generated_text2$generated_token_ids[1], is_a("integer"))
+  # textModelsRemove("gpt2")
+  expect_equal(generated_text8$generated_token_ids[1], 464)
+  expect_that(generated_text8$generated_token_ids[1], is_a("integer"))
 })
 
 
@@ -87,7 +207,7 @@ test_that("textNER test", {
   )
   ner_example2
   expect_equal(ner_example2$satisfactiontexts_NER$score[2], 0.976, tolerance = 0.01)
-  textModelsRemove("dslim/bert-base-NER")
+  # textModelsRemove("dslim/bert-base-NER")
 })
 
 test_that("textSum test", {
@@ -132,7 +252,7 @@ test_that("textZeroShot test", {
   )
 
   testthat::expect_equal(ZeroShot_example$scores_x_1[1], 0.3341856, tolerance = 0.00001)
-  textModelsRemove("okho0653/distilbert-base-uncased-zero-shot-sentiment-model")
+  # textModelsRemove("okho0653/distilbert-base-uncased-zero-shot-sentiment-model")
 })
 
 test_that("textTranslate test", {
@@ -153,5 +273,5 @@ test_that("textTranslate test", {
     translation_example$en_to_fr_satisfactiontexts[1],
     "Je ne suis pas satisfait de ma vie, je suis reconnaissante de ce que j'ai et de ce que je suis, car la situation peut toujours Ãªtre pire. Je veux une carriÃ¨re et un diplÃ´me, je veux perdre de poids et je n'ai pas encore atteint ces objectifs."
   )
-  textModelsRemove("t5-small")
+  # textModelsRemove("t5-small")
 })
```

I need your help:
1. Please make sure you understand what I did here (there is not much)
2. Please help me write a polite but no too extensive pull request message

----
### Pull Request: More Text Generation Parameters

**Description**: 

This pull request introduces several enhancements to the text generation functionality in the `text` package. The following changes have been made:

- **New Parameters**: Added `max_length`, `max_new_tokens`, `min_length`, and `min_new_tokens` to the `hgTransformerGetTextGeneration` function. These parameters provide users with greater control and are the ones I was missing for my specific use cases. Some minor message edits were also made.

- **R Interface Update**: Updated the `textGeneration` function in the R interface to accept the new parameters and ensure they are passed to the underlying Python function.

- **Expanded Tests**: Added comprehensive tests to validate the new functionality and ensure robustness.

Please note that while I have made these enhancements, the R-CMD-Check still runs into some errors and warnings that are not related to the proposed changes. I also encountered an issue when `nltk` failed to find `tokenizers/punkt_tab`, so I added a modified copy of `punk` downloading script, as a precaution. Additionally, the package authors are welcome to rearrange the new arguments as they see fit.

Thank you for considering this pull request!
